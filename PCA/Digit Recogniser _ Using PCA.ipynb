{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.decomposition import PCA\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/digit-recognizer/train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training dataset contains 784 columns, which denote the 28 * 28 pixel values of each number from 0 to 9. The \"label\" column is our target column. The pixel values populated from pixel0 to pixel783 contain integer values from 0 to 255, inclusive."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The values from 1 to 9 are almost uniformly distributed"},{"metadata":{},"cell_type":"markdown","source":"Separating the label values and column values in different datasets before commencing with data processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train[\"label\"]\nX=train.loc[:, train.columns != \"label\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before running any algorithm, we will scale the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_values = X.values\nX_std = StandardScaler().fit_transform(X_values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Breaking the data into training and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.25, random_state = 42, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_train: \", X_train.shape)\nprint(\"X_test: \", X_test.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"y_test: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will run the algorithm on the data without dimensionality reduction and then check the efficacy with a dimensionality reduction technique "},{"metadata":{},"cell_type":"markdown","source":"**Logictic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nlog  = LogisticRegression(random_state = 42, multi_class=\"multinomial\", solver=\"saga\", max_iter=200)\nstart_time = time.time()\nlog.fit(X_train, y_train)\nend_time = time.time()\ntime1 = end_time-start_time\nprint(\"Time elapsed: \",time1)\ny_pred = log.predict(X_test)\n\n# Accuracy Estimation\nprint('Accuracy Score (Train Data):', np.round(log.score(X_train, y_train), decimals = 3))\nprint('Accuracy Score (Test Data):', np.round(log.score(X_test, y_test), decimals = 3))\n\n# Classification Report\nlogistic_report = classification_report(y_test, y_pred)\nprint(logistic_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> It took about 269 seconds to fit the data with 784 columns. \nOverall accuracy achieved is 92%.\nHowever the model is slightly overfitting as the training accuracy is more than test accuracy."},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators = 300, max_depth = 5, random_state = 42, n_jobs = -1)\nstart_time = time.time()\nrfc.fit(X_train, y_train)\nend_time = time.time()\ntime1 = end_time-start_time\nprint(\"Time elapsed: \",time1)\ny_pred = rfc.predict(X_test)\n\n# Accuracy Estimation\nprint('Accuracy Score (Train Data):', np.round(rfc.score(X_train, y_train), decimals = 3))\nprint('Accuracy Score (Test Data):', np.round(rfc.score(X_test, y_test), decimals = 3))\n\n# Classification Report\nrandom_forest_report = classification_report(y_test, y_pred)\nprint(random_forest_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Classifier executed in about 10 seconds.\nHowever the accuracy is less than that of Logistic Regression\nWe might need to tune parameters to achieve a better performance"},{"metadata":{},"cell_type":"markdown","source":"**XGBClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = XGBClassifier(max_depth = 5, n_jobs = -1, objective='multi:softmax',num_class=10, eval_metric=\"mlogloss\", random_state = 42)\nstart_time = time.time()\nclf.fit(X_train, y_train)\nend_time = time.time()\ntime1 = end_time-start_time\nprint(time1)\ny_pred = clf.predict(X_test)\n\n# Accuracy Estimation\nprint('Accuracy Score (Train Data):', np.round(clf.score(X_train, y_train), decimals = 3))\nprint('Accuracy Score (Test Data):', np.round(clf.score(X_test, y_test), decimals = 3))\n\n# Classification Report\nXGB_report = classification_report(y_test, y_pred)\nprint(XGB_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGB Classifier executed in 899 seconds. Again the model is overfitting, as we achieved 100% accuracy in the training set, but 97.3 in the test set. "},{"metadata":{},"cell_type":"markdown","source":"We will now decrease the number of columns using Principal Component Analysis (PCA) and check performance and accuracy"},{"metadata":{},"cell_type":"markdown","source":"PCA is performed on a small subset of data so that we do not end up spending a lot of time on the analysis. Here we will fit the data on X_pca,which is just 30% of the entire dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split: PCA data and non PCA data\nX_data, X_pca = train_test_split(X_std, test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_pca=pd.DataFrame(X_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_pca","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will aim to explain 98% of the variance with PCA. We could reduce or increase it as per the needs of our project"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(0.98).fit(X_pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will draw an elbow plot to check the optimal number of features that can explain 98% of the variance in data"},{"metadata":{"trusted":true},"cell_type":"code","source":"var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\nplt.ylabel('% Variance Explained')\nplt.xlabel('Number of Features')\nplt.title('PCA Analysis')\nplt.ylim(30,100.5)\nplt.style.context('seaborn-whitegrid')\nplt.plot(var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('%d components explain 98%% of the variation in data' % pca.n_components_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just a little more than half of all 784 features can explain the intended variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=406, random_state = 0)\npca.fit(X_pca)\nX_pca_t = pca.transform(X_pca)\nprint(X_pca_t.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_std_t = pca.transform(X_std)\nprint(X_std_t.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The actual dataset X_std is PCA transformed"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_std_t = pd.DataFrame(data = X_std_t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_std_t","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We wil now repeat the steps from train_test_split to running the machine learning algorithms using the reduced dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test=train_test_split(X_std_t, y, test_size=0.25, random_state=42, stratify = y) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_train: \", X_train.shape)\nprint(\"X_test: \", X_test.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"y_test: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression on PCA reduced dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nlog  = LogisticRegression(random_state = 42, multi_class=\"multinomial\", solver=\"saga\", max_iter=200)\nstart_time = time.time()\nlog.fit(X_train, y_train)\nend_time = time.time()\ntime1 = end_time-start_time\nprint(\"Time elapsed: \",time1)\ny_pred = log.predict(X_test)\n\n# Accuracy Estimation\nprint('Accuracy Score (Train Data):', np.round(log.score(X_train, y_train), decimals = 3))\nprint('Accuracy Score (Test Data):', np.round(log.score(X_test, y_test), decimals = 3))\n\n# Classification Report\nlogistic_report = classification_report(y_test, y_pred)\nprint(logistic_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier on PCA reduced dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators = 300, max_depth = 5, random_state = 42, n_jobs = -1)\nstart_time = time.time()\nrfc.fit(X_train, y_train)\nend_time = time.time()\ntime1 = end_time-start_time\nprint(\"Time elapsed: \",time1)\ny_pred = rfc.predict(X_test)\n\n# Accuracy Estimation\nprint('Accuracy Score (Train Data):', np.round(rfc.score(X_train, y_train), decimals = 3))\nprint('Accuracy Score (Test Data):', np.round(rfc.score(X_test, y_test), decimals = 3))\n\n# Classification Report\nrandom_forest_report = classification_report(y_test, y_pred)\nprint(random_forest_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XGB Classifier on PCA reduced dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = XGBClassifier(max_depth = 5, n_jobs = -1, objective='multi:softmax',num_class=10, eval_metric=\"mlogloss\", random_state = 42)\nstart_time = time.time()\nclf.fit(X_train, y_train)\nend_time = time.time()\ntime1 = end_time-start_time\nprint(time1)\ny_pred = clf.predict(X_test)\n\n# Accuracy Estimation\nprint('Accuracy Score (Train Data):', np.round(clf.score(X_train, y_train), decimals = 3))\nprint('Accuracy Score (Test Data):', np.round(clf.score(X_test, y_test), decimals = 3))\n\n# Classification Report\nXGB_report = classification_report(y_test, y_pred)\nprint(XGB_report)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}